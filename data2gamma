#!/bin/python

'''
                    Copyright Oliver Kowalke 2020.
    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with this program.  If not, see <http://www.gnu.org/licenses/>.
'''

import abc
import argparse
import ast
import configparser
import json
import matplotlib.patches as mpatches
import matplotlib.pyplot as plt
import numpy as np
import sys
import pandas as pd

from functools import reduce
from math import log10
from matplotlib.ticker import MultipleLocator
from pathlib import Path
from sklearn.linear_model import RANSACRegressor
from sklearn.ensemble import IsolationForest
from sklearn.covariance import EllipticEnvelope
from sklearn.pipeline import make_pipeline
from sklearn.kernel_approximation import Nystroem
from sklearn.linear_model import SGDOneClassSVM


def outliers_mask(x, y):
    # return the gradient of an N-dimensional array
    # example:
    # array([ 1. ,  1.5,  2.5,  3.5,  4.5,  5. ])
    # gradient is defined as (change in y)/(change in x)
    # x, is the list index, so the difference between adjacent values is 1
    # at the boundaries, the first difference is calculated
    # this means that at each end of the array, the gradient given is simply,
    # the difference between the end two values (divided by 1)
    # away from the boundaries the gradient for a particular index is given by
    # taking the difference between the the values either side and dividing by 2
    # j[0] = (y[1]-y[0])/1 = (2-1)/1  = 1
    # j[1] = (y[2]-y[0])/2 = (4-1)/2  = 1.5
    # j[2] = (y[3]-y[1])/2 = (7-2)/2  = 2.5
    # j[3] = (y[4]-y[2])/2 = (11-4)/2 = 3.5
    # j[4] = (y[5]-y[3])/2 = (16-7)/2 = 4.5
    # j[5] = (y[5]-y[4])/1 = (16-11)/1 = 5
    dy = np.gradient(y, x)

    # reshape(-1, 1), asking numpy to reshape the array with 1 column and as many
    # rows as necessary to accommodate the data
    # this operation will result in a 2D array with a shape (n, 1), where n is the
    # number of elements in your original array
    # example:
    # # Original array
    # arr = np.array([1, 2, 3, 4, 5, 6])
    # print('Original array shape:', arr.shape)
    #
    # # Reshape array
    # reshaped_arr = arr.reshape(-1, 1)
    # print('Reshaped array shape:', reshaped_arr.shape)
    #
    # Output:
    #
    # Original array shape: (6,)
    # Reshaped array shape: (6, 1)

    # IsolationForest:
    # contamination == proportion of outliers in the data set
    # random_state == controls the pseudo-randomness of the selection of the
    #                 feature and split values for each branching step and
    #                 each tree in the forest.amount of
#    rgr = IsolationForest(contamination=0.2, random_state=42)
#    rgr = EllipticEnvelope(contamination=0.2, random_state=42)
#    rgr =  make_pipeline(
#            Nystroem(gamma=0.1, random_state=42, n_components=21),
#            SGDOneClassSVM(
#                nu=0.2,
#                shuffle=True,
#                fit_intercept=True,
#                random_state=42,
#                tol=1e-6,
#            )
#        )
    # predict(): gets a binary prediction for each record
    # flatten(): return a copy of the array collapsed into one dimension
    dy_dt = rgr.fit(x.reshape(-1, 1), dy.reshape(-1, 1)).predict(x.reshape(-1, 1)).flatten()

    msk = np.array([])
    for y in np.unique(dy_dt):
        # compare unique element 'y' of predictions 'dy_dt'
        msk_ = dy_dt == y
        # choose the list with most elements
        if (np.count_nonzero(msk) < np.count_nonzero(msk_)):
            msk = msk_
    return msk


class Model(abc.ABC):
    _min_density = 0.17
    _max_density = 1.37
    _msk = np.array([])

    @abc.abstractmethod
    def _process(self):
        'process data'

    def __init__(self, data):
        self._exposure, self._density = zip(*data)
        self._exposure = np.array(self._exposure)
        self._density = np.array(self._density)
        self._msk = outliers_mask(self._exposure, self._density)
        self._process()

    @property
    def exposure(self):
        return self._exposure

    @property
    def density(self):
        return self._density

    @property
    def min_density(self):
        return self._min_density

    @property
    def max_density(self):
        return self._max_density

    @property
    def exposure_at_min_density(self):
        return self._exposure_at_min_density

    @property
    def exposure_at_max_density(self):
        return self._exposure_at_max_density

    @property
    def null_point(self):
        return self._null_point

    @property
    def gamma(self):
        return self._gamma

    @property
    def r_square(self):
        return self._r2

    @property
    def lower_offset(self):
        return 0.1

    @property
    def msk(self):
        return self._msk

    def predict(self, x):
        return self._model.predict(x)


'metric based on: Lambrecht/Woodhouse, Way Beyond Monochrome'
class RANSACRegressionModel(Model):
    def __init__(self, data):
        super(RANSACRegressionModel, self).__init__(data)

    def _process(self):
        self._model = RANSACRegressor().fit(self.exposure[self.msk].reshape(-1, 1), self.density[self.msk].reshape(-1, 1))
        self._r2 = round(self._model.score(self.exposure[self.msk].reshape(-1, 1), self.density[self.msk].reshape(-1, 1)), 4)
        self._gamma = self._model.estimator_.coef_[0][0]
        # exposure exposure_at_mindensity at min density = 0.17
        self._exposure_at_min_density = round((0.17 - self._model.estimator_.intercept_[0])/self._gamma, 2)
        self._exposure_at_max_density = round((1.37 - self._model.estimator_.intercept_[0])/self._gamma, 2)
        self._null_point = round((0 - self._model.estimator_.intercept_[0])/self._gamma, 2)


def relative_exposure(density):
    return 10 * log10(2) - density


def average_density(fog, data):
    data = [(exposure, ast.literal_eval(density)) for (exposure, density) in data]
    data = data[::-1]
    return [(round(relative_exposure(float(exposure)), 3), round(round(reduce(lambda a, b: float(a) + float(b), density) / len(density), 3) - fog, 3)) for exposure, density in data]


def parse_data(file_p):
    config = configparser.ConfigParser()
    config.read(str(file_p))
    meta = config['META']
    data = config.items('DATA')
    data = average_density(float(meta['fog']), data)
    return meta, data


def zone_from_density(density):
    return 10 - ( (10 * log10(2) - density) / log10(2))


def evaluate(file_p, model, meta):
    fig, ax = plt.subplots()
    ax.set_title('{}: {}@{} {}[{}] {}/{} {}'.format(
        'Lambrecht/Woodhouse',
        meta['film'],
        meta['ISO'],
        meta['developer'],
        meta['dilution'],
        meta['time'],
        meta['temperature'],
        meta['aggitation']),
        fontsize=10)
    ax.grid(which='both')
    ax.scatter(model.exposure[model.msk], model.density[model.msk], marker='+', color='limegreen')
    ax.scatter(model.exposure[np.invert(model.msk)], model.density[np.invert(model.msk)], marker='+', color='black')
    ax.scatter(model.exposure_at_min_density, model.min_density, marker='.', color='red', label='exposure(Dmin)={}'.format(model.exposure_at_min_density))
    ax.scatter(model.exposure_at_max_density, model.max_density, marker='.', color='red', label='exposure(Dmax)={}'.format(model.exposure_at_max_density))
    line = np.linspace(model.null_point, max(model.exposure_at_max_density, model.exposure[-1]), 200)
    ax.plot(line, model._model.predict(line.reshape(-1, 1)), linewidth=1.0, label='linear regression', color='blue')
    ax.xaxis.set_major_locator(MultipleLocator(0.5))
    ax.yaxis.set_major_locator(MultipleLocator(0.1))
    ax.set_xlabel('exposure')
    ax.set_ylabel('density')
    ax.legend(loc='upper left', bbox_to_anchor=(0.01, 0.99), borderaxespad=0.)
    handles, labels = ax.get_legend_handles_labels()
    desc = 'gamma = {}\nr^2= {}'.format(
        round(model.gamma, 2),
        model.r_square)
    handles.append(mpatches.Patch(color='none', label=desc))
    plt.legend(handles=handles, fontsize=8)
    file_p = file_p.with_suffix('.png')
    plt.savefig(str(file_p), dpi=300)
    tm = meta['time'].split(':')
    tm = round(float(tm[0]) + int(tm[1])/60, 1)
    return (tm, round(model.gamma, 2), round(model.exposure_at_min_density, 2), round(zone_from_density(model.exposure_at_min_density), 1))


def serialize(result):
    print(json.dumps(result))


def main(file):
    meta, data = parse_data(file)
    result = evaluate(file, RANSACRegressionModel(data), meta)
    serialize(result)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--file', type=str)
    args = parser.parse_args()
    file = Path(args.file).resolve()
    assert file.exists()
    main(file)
